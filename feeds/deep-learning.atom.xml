<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Connecting Dots - Deep Learning</title><link href="https://yardstick17.github.io/" rel="alternate"></link><link href="https://yardstick17.github.io/feeds/deep-learning.atom.xml" rel="self"></link><id>https://yardstick17.github.io/</id><updated>2017-06-01T00:00:00+05:30</updated><entry><title>Understanding CNN (Part 2)</title><link href="https://yardstick17.github.io/understanding-cnn-part-2.html" rel="alternate"></link><published>2017-06-01T00:00:00+05:30</published><updated>2017-06-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-06-01:/understanding-cnn-part-2.html</id><summary type="html">&lt;p&gt;This is the continuation of Part-1 of this series. if you are just started in the subject please review
the &lt;a href="http://amitkushwaha.co.in/understanding-cnn.html"&gt;Part-1&lt;/a&gt; of this series.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the continuation of Part-1 of this series. if you are just started in the subject please review
the &lt;a href="http://amitkushwaha.co.in/understanding-cnn.html"&gt;Part-1&lt;/a&gt; of this series.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are starting with this truth-table to train our perceptron model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;X&lt;/strong&gt;: Feature Vector of each sample&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Y&lt;/strong&gt;: Label for each sample&lt;/p&gt;
&lt;p&gt;&lt;img alt="weights-update-process-visualization" src="../images/cnn-weights-update-animation.gif"&gt;&lt;/p&gt;
&lt;p&gt;We initialize our perceptron model with random values of weights.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As explained in the Training Algorithm section, we aim to increase the performance at tasks in T.
In each epoch, we make a feed-forward our perceptron to predict the desired output. When the
predicted(calculated) is not consistent with the desired, we make an update to the weights of perceptron.
 Updating the weights means, wither the values of W are going to be increased or decreaded.
 This delta change is what &lt;strong&gt;&lt;em&gt;Training Algorithm&lt;/em&gt;&lt;/strong&gt; dictates in order to minimize the overall error/loss as defined.&lt;/p&gt;
&lt;p&gt;While we train any model, we always have a loss defined over which the weights are optimized.
A very typical training loss should appear like this.&lt;/p&gt;
&lt;p&gt;Learning rate is the momentum we give to our training speed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In our case we have defined our loss as the squared error. As explained in the previous 
session, the delta weights update formulates to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;delta_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="sd"&gt;    :param eta: Learning rate&lt;/span&gt;
&lt;span class="sd"&gt;    :param true_label:&lt;/span&gt;
&lt;span class="sd"&gt;    :param predicted_label:&lt;/span&gt;
&lt;span class="sd"&gt;    :param x:&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;lambda_param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;delta_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lambda_param&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted_label&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;true_label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;delta_w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script src="https://gist.github.com/yardstick17/dcf240784bcdf455a534b6d4179cd9c1.js"&gt;&lt;/script&gt;

&lt;p&gt;So, we have hyper-parameter to choose when we start training a model. In our case, hyper-parameters are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;eta: learning rate of the training&lt;/span&gt;
&lt;span class="sd"&gt;lambda_param: to limit the maximum delta change in weights&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With different learning rates we may get different loss-plots. If the learning rate is high, weights updates are
large, which &lt;em&gt;&lt;strong&gt;may&lt;/strong&gt;&lt;/em&gt; result in faster training.&lt;/p&gt;
&lt;p&gt;With Learning Rate,&lt;strong&gt;eta = 0.0001&lt;/strong&gt;, the training took more epochs to reach the desired minimum loss.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_e_0.001.png"&gt;&lt;/p&gt;
&lt;p&gt;When Learning Rate,&lt;strong&gt;eta = 0.01&lt;/strong&gt;, training speeds up almost with the factor of 10.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_plot_e_0.01.png"&gt;&lt;/p&gt;</content></entry><entry><title>Understanding CNN</title><link href="https://yardstick17.github.io/understanding-cnn.html" rel="alternate"></link><published>2017-05-01T00:00:00+05:30</published><updated>2017-05-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-05-01:/understanding-cnn.html</id><summary type="html">&lt;p&gt;Let's start with the most basic element of Neural Network - &lt;strong&gt;Perceptron&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The elementary entity and basic form of neural network which can also learn. 
The development owes to a biological inspiration from a Neuron. Much like Neuron, a perceptron 
takes an input signal, process it, and stimulate a response …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's start with the most basic element of Neural Network - &lt;strong&gt;Perceptron&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The elementary entity and basic form of neural network which can also learn. 
The development owes to a biological inspiration from a Neuron. Much like Neuron, a perceptron 
takes an input signal, process it, and stimulate a response. A single perceptron can learn to discriminate linear separable problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.007.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Representing a Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The more practical representation of Perceptron and its function.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.008.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Feature Vector&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Machine learning tasks are usually described in terms of how the machine
learning system should process an example. An example is a collection of features
that have been quantitatively measured from some object or event that we want
the machine learning system to process. We typically represent an example as a
vector x ∈ Rn where each entry xi of the vector is another feature. For example,
the features of an image are usually the values of the pixels in the image.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this type of task, the computer program is asked to specify
which of k categories some input belongs to. To solve this task, the learning
algorithm is usually asked to produce a function f : Rn → {1, . . . , k}. When
y = f (x), the model assigns an input described by vector x to a category
identified by numeric code y. There are other variants of the classification
task, for example, where f outputs a probability distribution over classes.
An example of a classification task is object recognition, where the input
is an image (usually described as a set of pixel brightness values), and the
output is a numeric code identifying the object in the image. For example,
the Willow Garage PR2 robot is able to act as a waiter that can recognize
different kinds of drinks and deliver them to people on command (Goodfellow
et al., 2010). Modern object recognition is best accomplished with
deep learning (Krizhevsky et al., 2012; Ioffe and Szegedy, 2015). Object
recognition is the same basic technology that allows computers to recognize
faces (Taigman et al., 2014), which can be used to automatically tag people
in photo collections and allow computers to interact more naturally with
their users.&lt;/p&gt;
&lt;p&gt;In this presentation, we are going to understand how perceptron learns actually.
To ease the purpose of understanding, we are going to learn this naive pattern.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Algorithm&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A machine learning algorithm is an algorithm that is able to learn from data. But
what do we mean by learning? Mitchell (1997) provides the definition “A computer
program is said to learn from experience E with respect to some class of tasks T
and performance measure P , if its performance at tasks in T , as measured by P ,
improves with experience E.” One can imagine a very wide variety of experiences
E, tasks T , and performance measures P , and we do not make any attempt in this
book to provide a formal definition of what may be used for each of these entities.
Instead, the following sections provide intuitive descriptions and examples of the
different kinds of tasks, performance measures and experiences that can be used
to construct machine learning algorithms.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.015.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Taking aforementioned mathematical genius to our purview. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Epoch&lt;/h3&gt;
&lt;p&gt;An epoch is defined as a single pass through your entire training set while training a machine 
learning model. In a single epoch, all training samples are presented to your model 
once. So the total number of epochs in training a model gives the 
number of cycles through the entire training datasest.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="weights-update-process-visualization" src="../images/cnn-weights-update-animation.gif"&gt;
As explained in the Training Algorithm section, we aim to increase the performance at tasks in T.
In each epoch, we make a feed-forward our perceptron to predict the desired output. When the
predicted(calculated) is not consistent with the desired, we make an update to the weights of perceptron.
 Updating the weights means, wither the values of W are going to be increased or decreaded.
 This delta change is what &lt;strong&gt;&lt;em&gt;Training Algorithm&lt;/em&gt;&lt;/strong&gt; dictates in order to minimize the overall error/loss as defined.&lt;/p&gt;
&lt;p&gt;While we train any model, we always have a loss defined over which the weights are optimized.
A very typical training loss should appear like this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_e_0.001.png"&gt;&lt;/p&gt;
&lt;p&gt;That's all for the theory, we will move to code in next session, &lt;a href="http://amitkushwaha.co.in/understanding-cnn-part-2.html"&gt;Understanding CNN - Part 2&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Image Aesthetic Assessment</title><link href="https://yardstick17.github.io/image-aesthetic-assessment.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/image-aesthetic-assessment.html</id><summary type="html">&lt;p&gt;Image Aesthetic Assessment using Deep Learning. The defined network takes the complete image instead of fixed or re-sized input. This makes the network to learn about original image composition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;"Beauty is really in the eye of the beholder"&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Image aesthetics assessment is an attempt to define the &lt;strong&gt;beauty&lt;/strong&gt; of an Image.
While everyone has different tastes, there are universally accepted norms when it comes to beauty – things which everyone pretty much agrees are beautiful, like sunsets or sunrises over the mountains or the ocean.&lt;/p&gt;
&lt;p&gt;Some of the visual features that come handy are &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edge distributions, &lt;/li&gt;
&lt;li&gt;color histogram, &lt;/li&gt;
&lt;li&gt;Some photographic rules like rule of thirds also determines the beauty of an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Defining image quality with visual features like other manually curated features are limited in the scope.&lt;/p&gt;
&lt;p&gt;The two photographer's story.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Great Shot!!&lt;/th&gt;
&lt;th&gt;So what?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Beautiful Image" src="./../images/resize_thumb_ambiance_amazing_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="low quality Image" src="./../images/resize_thumb_ambiance_bad_1024.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The Image is of the same place taken with different lightning, angle, adjusted contrast. And it is obvious that image on the left has better aesthetic attire.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Significance of Image Aesthetics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For a platform especially that serves media content, one of the crucial aspects is to show high-quality content. With social sites and the given ‘selfie’ trend, we are generating a huge amount of data in the form of either images or videos.
 Having a track on the quality will always be helpful.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Curated Content&lt;/th&gt;
&lt;th&gt;User Generated Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Curated Content" src="./../images/resize_thumb_Screen_Shot_2017_04_15_at_11_18_25_AM_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="User Generated Content" src="./../images/resize_thumb_Screen_Shot_2017_04_15_at_11_30_00_AM_1024.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;strong&gt;Can we model such Human Perception?&lt;/strong&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;strong&gt;Deep learning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The topic needs no introduction. It’s a revolution especially in the image classification domain since the last 5 years. With “Alexnet” winning the Image-Net competition, improving error rate with a huge margin acted as a spark in the field. Since then, CNN has many state of the arts on its name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network architecture of Alexnet.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alexnet architecture" src="./../images/thumb_CNN_image_1024.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The first layer is input, where input is fed to the network. We can see their pooling operations, convolution operations finally followed by a fully connected layer 
and final softmax layer so that we get values as the probability for each class we label.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Fixed size input constraint&lt;/strong&gt;&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Input Layer&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Input Layer" src="./../images/thumb_input_CNN_image_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;Input image &lt;strong&gt;re-sized to 224 * 224&lt;/strong&gt; irrespective of original image shape.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We always resize the input feature vector. If the image is larger, image is cropped 
or pad image if image dimensions are smaller, to get a fixed size input to feed the network&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;The Mountains&lt;/th&gt;
&lt;th&gt;Qutub Minar&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Mountains" src="./../images/iceland-blue-lagoon-and-snow-mountains-landscape-header.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Qutub Minar" src="./../images/qutub-minar-minaret.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The above two images are &lt;strong&gt;beautiful in their original aspect ratio.&lt;/strong&gt; 
What happens if we re-size the image to a fixed size of 224 * 224? 
Certainly, the image will &lt;strong&gt;loose all it’s original aesthetic value!&lt;/strong&gt; 
From Landscape to Squared size. All damage is done. The original image composition is lost when an image is re-sized.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Demystifying the Network Architecture&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Network Architecture" src="./../images/thumb_CNN_image_1024.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Let’s unveil the hidden layers! So, we can see that after the input, there are few layers of &lt;strong&gt;Operations&lt;/strong&gt;. 
The operations are either &lt;strong&gt;Max-pooling&lt;/strong&gt; or convolving with a filter i.e. &lt;strong&gt;Convolution&lt;/strong&gt;.
So why the fixed size of input is required at all then?&lt;br&gt;
It’s because of the &lt;strong&gt;Fully Connected Layer&lt;/strong&gt; just before the outputs. 
Fully Connected Layers are in the network for the non-linear combination of feature extracted before in convolution network.&lt;/p&gt;
&lt;p&gt;Let's understand bit by bit.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Max Pooling&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Max pooling are there for &lt;strong&gt;Down-sampling&lt;/strong&gt; the feature space while maintaining the spatial information
Max Pooling in action&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max Pooling" src="./../images/cropped_max_pooling.gif"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Spatial Pyramid Pooling&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In spp, an image is divided into bins. Each bin is pooled in its turn. As the number of bins is fixed, 
we always get the &lt;strong&gt;Fixed Shape Output&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Spp operation in action&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Pyramid Pooling" src="./../images/sppp_2_cropped.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Pyramid Pooling" src="./../images/bin_4_spp.gif"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Spp Network Architecture&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Spp Network Architecture" src="./../images/network.png"&gt;&lt;/p&gt;
&lt;p&gt;The first network is the traditional CNN, we can see the &lt;strong&gt;Max-pool layer&lt;/strong&gt; just before the fully connected layer.
In the second architecture, the last max pooling layer is &lt;strong&gt;replaced by a Spp layer&lt;/strong&gt;. 
With the &lt;strong&gt;Fixed Bin size (1,2,4)&lt;/strong&gt; we make sure that the fully connected layer gets the fixed shape input.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spp Network Architecture" src="./../images/687474703a2f2f692e696d6775722e636f6d2f5351574a566f442e706e67.png"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training the Spp-Net&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Training the Spp-Net on &lt;strong&gt;live-dataset&lt;/strong&gt;, very small dataset, about 1K images total, model achieved the accuracy of 75% on training data, 83% of the test data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Training Loss&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Spp-Net Accuracy" src="./../images/training_accuracy.png"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Spp-Net Training Loss" src="./../images/training_loss.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;With Spp in Network&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model learns the scale invariant feature like SIFT(traditional image processing algorithm).&lt;/li&gt;
&lt;li&gt;One of the challenges in text classification with Deep learning is the fixed size feature vector representation of the sentence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Interesting Results&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After training model, I experimented with few results. These are the most interesting and promising results I found.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Blurred Cropped Image&lt;/th&gt;
&lt;th&gt;Complete Image&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Blurred Image" src="./../images/low_quality_blurred_image.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Complete Image" src="./../images/high_quality_blurred_image.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I recently came across this &lt;a href="https://philliphaumesserphotography.com"&gt;Photographer&lt;/a&gt; experience from being an amateur to professional.&lt;br&gt;
He proved what difference a change in perspective can make. So, I decided to make my trained model judge for his efficacy.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amateur Click? Yes, it is.&lt;/th&gt;
&lt;th&gt;Pro Click? I am already amazed.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Beautiful Image" src="../images/amateur_photographer.png"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="low quality Image" src="../images/professionl_photographer.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The aesthetic trained model has passed him with flying colors in Photographic skills. Well done Phillip Haumesser.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;SPP Network&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;script src="https://gist.github.com/yardstick17/22c02363c5e04763373b588f1a3bceeb.js"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1406.4729"&gt;Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/document/7780429/"&gt;Long Mai, Hailin Jin, Feing Liu, Composition-Preserving Deep Photo Aesthetics Assessment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that, I would like to wrap up. Any Questions?&lt;/p&gt;</content><category term="image quality prediction"></category><category term="spatial pyramid pooling"></category><category term="spp-net"></category><category term="deep learning"></category></entry><entry><title>Image Classification</title><link href="https://yardstick17.github.io/image-classification.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/image-classification.html</id><summary type="html">&lt;p&gt;A Deep learning approach to classify image into four categories - Food, Ambiance, Menu and Human.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Deep learning approach to classify image into four categories - Food, Ambiance, Menu and Human.&lt;/p&gt;</content></entry><entry><title>Image Similarity</title><link href="https://yardstick17.github.io/image-similarity.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/image-similarity.html</id><summary type="html">&lt;p&gt;A Deep learning approach to identify a probable image plagiarism.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Deep learning approach to identify a probable image plagiarism.&lt;/p&gt;</content></entry><entry><title>Optimizers in Deep Learning</title><link href="https://yardstick17.github.io/optimizers-in-deep-learning.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/optimizers-in-deep-learning.html</id><summary type="html">&lt;p&gt;The fundamental of any Machine learning models is defining a cost function or loss function. Optimizing over data-set i.e. minimizing our
cost function or loss enables us to learn the model. The technique of efficiently minimizing our cost function is done using the
well defined algorithms under the premise …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The fundamental of any Machine learning models is defining a cost function or loss function. Optimizing over data-set i.e. minimizing our
cost function or loss enables us to learn the model. The technique of efficiently minimizing our cost function is done using the
well defined algorithms under the premise of Optimizers.  &lt;/p&gt;</content></entry></feed>