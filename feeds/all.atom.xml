<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Connecting Dots</title><link href="https://yardstick17.github.io/" rel="alternate"></link><link href="https://yardstick17.github.io/feeds/all.atom.xml" rel="self"></link><id>https://yardstick17.github.io/</id><updated>2017-06-01T00:00:00+05:30</updated><entry><title>Understanding CNN (Part 2)</title><link href="https://yardstick17.github.io/understanding-cnn-part-2.html" rel="alternate"></link><published>2017-06-01T00:00:00+05:30</published><updated>2017-06-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-06-01:/understanding-cnn-part-2.html</id><summary type="html">&lt;p&gt;This is the continuation of Part-1 of this series. if you are just started in the subject please review
the &lt;a href="http://amitkushwaha.co.in/understanding-cnn.html"&gt;Part-1&lt;/a&gt; of this series.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the continuation of Part-1 of this series. if you are just started in the subject please review
the &lt;a href="http://amitkushwaha.co.in/understanding-cnn.html"&gt;Part-1&lt;/a&gt; of this series.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are starting with this truth-table to train our perceptron model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;X&lt;/strong&gt;: Feature Vector of each sample&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Y&lt;/strong&gt;: Label for each sample&lt;/p&gt;
&lt;p&gt;&lt;img alt="weights-update-process-visualization" src="../images/cnn-weights-update-animation.gif"&gt;&lt;/p&gt;
&lt;p&gt;We initialize our perceptron model with random values of weights.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As explained in the Training Algorithm section, we aim to increase the performance at tasks in T.
In each epoch, we make a feed-forward our perceptron to predict the desired output. When the
predicted(calculated) is not consistent with the desired, we make an update to the weights of perceptron.
 Updating the weights means, wither the values of W are going to be increased or decreaded.
 This delta change is what &lt;strong&gt;&lt;em&gt;Training Algorithm&lt;/em&gt;&lt;/strong&gt; dictates in order to minimize the overall error/loss as defined.&lt;/p&gt;
&lt;p&gt;While we train any model, we always have a loss defined over which the weights are optimized.
A very typical training loss should appear like this.&lt;/p&gt;
&lt;p&gt;Learning rate is the momentum we give to our training speed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In our case we have defined our loss as the squared error. As explained in the previous 
session, the delta weights update formulates to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;delta_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="sd"&gt;    :param eta: Learning rate&lt;/span&gt;
&lt;span class="sd"&gt;    :param true_label:&lt;/span&gt;
&lt;span class="sd"&gt;    :param predicted_label:&lt;/span&gt;
&lt;span class="sd"&gt;    :param x:&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;lambda_param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;delta_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lambda_param&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted_label&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;true_label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;delta_w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script src="https://gist.github.com/yardstick17/dcf240784bcdf455a534b6d4179cd9c1.js"&gt;&lt;/script&gt;

&lt;p&gt;So, we have hyper-parameter to choose when we start training a model. In our case, hyper-parameters are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;eta: learning rate of the training&lt;/span&gt;
&lt;span class="sd"&gt;lambda_param: to limit the maximum delta change in weights&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With different learning rates we may get different loss-plots. If the learning rate is high, weights updates are
large, which &lt;em&gt;&lt;strong&gt;may&lt;/strong&gt;&lt;/em&gt; result in faster training.&lt;/p&gt;
&lt;p&gt;With Learning Rate,&lt;strong&gt;eta = 0.0001&lt;/strong&gt;, the training took more epochs to reach the desired minimum loss.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_e_0.001.png"&gt;&lt;/p&gt;
&lt;p&gt;When Learning Rate,&lt;strong&gt;eta = 0.01&lt;/strong&gt;, training speeds up almost with the factor of 10.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_plot_e_0.01.png"&gt;&lt;/p&gt;</content></entry><entry><title>Understanding CNN</title><link href="https://yardstick17.github.io/understanding-cnn.html" rel="alternate"></link><published>2017-05-01T00:00:00+05:30</published><updated>2017-05-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-05-01:/understanding-cnn.html</id><summary type="html">&lt;p&gt;Let's start with the most basic element of Neural Network - &lt;strong&gt;Perceptron&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The elementary entity and basic form of neural network which can also learn. 
The development owes to a biological inspiration from a Neuron. Much like Neuron, a perceptron 
takes an input signal, process it, and stimulate a response …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's start with the most basic element of Neural Network - &lt;strong&gt;Perceptron&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The elementary entity and basic form of neural network which can also learn. 
The development owes to a biological inspiration from a Neuron. Much like Neuron, a perceptron 
takes an input signal, process it, and stimulate a response. A single perceptron can learn to discriminate linear separable problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.007.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Representing a Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The more practical representation of Perceptron and its function.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.008.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Feature Vector&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Machine learning tasks are usually described in terms of how the machine
learning system should process an example. An example is a collection of features
that have been quantitatively measured from some object or event that we want
the machine learning system to process. We typically represent an example as a
vector x ∈ Rn where each entry xi of the vector is another feature. For example,
the features of an image are usually the values of the pixels in the image.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this type of task, the computer program is asked to specify
which of k categories some input belongs to. To solve this task, the learning
algorithm is usually asked to produce a function f : Rn → {1, . . . , k}. When
y = f (x), the model assigns an input described by vector x to a category
identified by numeric code y. There are other variants of the classification
task, for example, where f outputs a probability distribution over classes.
An example of a classification task is object recognition, where the input
is an image (usually described as a set of pixel brightness values), and the
output is a numeric code identifying the object in the image. For example,
the Willow Garage PR2 robot is able to act as a waiter that can recognize
different kinds of drinks and deliver them to people on command (Goodfellow
et al., 2010). Modern object recognition is best accomplished with
deep learning (Krizhevsky et al., 2012; Ioffe and Szegedy, 2015). Object
recognition is the same basic technology that allows computers to recognize
faces (Taigman et al., 2014), which can be used to automatically tag people
in photo collections and allow computers to interact more naturally with
their users.&lt;/p&gt;
&lt;p&gt;In this presentation, we are going to understand how perceptron learns actually.
To ease the purpose of understanding, we are going to learn this naive pattern.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Algorithm&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A machine learning algorithm is an algorithm that is able to learn from data. But
what do we mean by learning? Mitchell (1997) provides the definition “A computer
program is said to learn from experience E with respect to some class of tasks T
and performance measure P , if its performance at tasks in T , as measured by P ,
improves with experience E.” One can imagine a very wide variety of experiences
E, tasks T , and performance measures P , and we do not make any attempt in this
book to provide a formal definition of what may be used for each of these entities.
Instead, the following sections provide intuitive descriptions and examples of the
different kinds of tasks, performance measures and experiences that can be used
to construct machine learning algorithms.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.015.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Taking aforementioned mathematical genius to our purview. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Perceptron" src="../images/TalkCNN.016.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;Epoch&lt;/h3&gt;
&lt;p&gt;An epoch is defined as a single pass through your entire training set while training a machine 
learning model. In a single epoch, all training samples are presented to your model 
once. So the total number of epochs in training a model gives the 
number of cycles through the entire training datasest.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training Perceptron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="weights-update-process-visualization" src="../images/cnn-weights-update-animation.gif"&gt;
As explained in the Training Algorithm section, we aim to increase the performance at tasks in T.
In each epoch, we make a feed-forward our perceptron to predict the desired output. When the
predicted(calculated) is not consistent with the desired, we make an update to the weights of perceptron.
 Updating the weights means, wither the values of W are going to be increased or decreaded.
 This delta change is what &lt;strong&gt;&lt;em&gt;Training Algorithm&lt;/em&gt;&lt;/strong&gt; dictates in order to minimize the overall error/loss as defined.&lt;/p&gt;
&lt;p&gt;While we train any model, we always have a loss defined over which the weights are optimized.
A very typical training loss should appear like this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Training loss Graph" src="../images/training_loss_e_0.001.png"&gt;&lt;/p&gt;
&lt;p&gt;That's all for the theory, we will move to code in next session, &lt;a href="http://amitkushwaha.co.in/understanding-cnn-part-2.html"&gt;Understanding CNN - Part 2&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Distributed Representations of Words and Phrases and their Composition</title><link href="https://yardstick17.github.io/distributed-representations-of-words-and-phrases-and-their-composition.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/distributed-representations-of-words-and-phrases-and-their-composition.html</id><summary type="html">&lt;p&gt;&lt;a href="http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf"&gt;Paper Link&lt;/a&gt;
&lt;strong&gt;Authors:&lt;/strong&gt; Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean
&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper is second in the series by Mikolov et al. of &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2009/mikolov_ic2009_nnlm_4.pdf"&gt;Statistical Language Models Based on Neural Networks&lt;/a&gt;
It is an extension to the proposed Skip-gram model in terms of quality and training speed. It …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf"&gt;Paper Link&lt;/a&gt;
&lt;strong&gt;Authors:&lt;/strong&gt; Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean
&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper is second in the series by Mikolov et al. of &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2009/mikolov_ic2009_nnlm_4.pdf"&gt;Statistical Language Models Based on Neural Networks&lt;/a&gt;
It is an extension to the proposed Skip-gram model in terms of quality and training speed. It also incorporates methods for Phrase representation and explores the 
interesting property of Skip-gram model.&lt;/p&gt;</content></entry><entry><title>Hierarchical Probabilistic Neural Network Language Model</title><link href="https://yardstick17.github.io/hierarchical-probabilistic-neural-network-language-model.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/hierarchical-probabilistic-neural-network-language-model.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf"&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Authors: &lt;span&gt; Frederic Morin, Yoshua Bengio &lt;/span&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Various neural network architecture have been used for Language modeling. With the emergence of word embeddings, these models
have been successfully applied. However these models are quite slow in comparison to traditional n-gram models.
To speed-up training and prediction, hierarchical the decomposition …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf"&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Authors: &lt;span&gt; Frederic Morin, Yoshua Bengio &lt;/span&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;strong&gt;Aim&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Various neural network architecture have been used for Language modeling. With the emergence of word embeddings, these models
have been successfully applied. However these models are quite slow in comparison to traditional n-gram models.
To speed-up training and prediction, hierarchical the decomposition of the conditional probabilities that yields a speed-up by a factor about 200.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In n-gram model as the size of vocabulary increases, the dictionary sizes with all combinations shoots up very sharply.&lt;/p&gt;
&lt;h3&gt;Challenges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traversal across all combination even if a small fraction of update is done.&lt;/li&gt;
&lt;li&gt;Similar object should have the same probability which is not possible if knowledge free representation is used.&lt;/li&gt;
&lt;li&gt;Long training time and huge computation is needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;strong&gt;The objective of this paper is thus to propose a much faster variant of the neural probabilistic language model.&lt;/strong&gt;&lt;/h5&gt;</content></entry><entry><title>Image Aesthetic Assessment</title><link href="https://yardstick17.github.io/image-aesthetic-assessment.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/image-aesthetic-assessment.html</id><summary type="html">&lt;p&gt;Image Aesthetic Assessment using Deep Learning. The defined network takes the complete image instead of fixed or re-sized input. This makes the network to learn for original image composition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;"Beauty is really in the eye of the beholder"&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Image aesthetics assessment is an attempt to define the &lt;strong&gt;beauty&lt;/strong&gt; of an Image.
While everyone has different tastes, there are universally accepted norms when it comes to beauty – things which everyone pretty much agrees are beautiful, like sunsets or sunrises over the mountains or the ocean.&lt;/p&gt;
&lt;p&gt;Some of the visual feature that come handy are &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edge distributions, &lt;/li&gt;
&lt;li&gt;color histogram, &lt;/li&gt;
&lt;li&gt;Some photographic rules like rue of thirds also determines the beauty of an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Defining image quality with visual features like other manually curated features are limited in the scope.&lt;/p&gt;
&lt;p&gt;The two photographer's story.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Great Shot!!&lt;/th&gt;
&lt;th&gt;So what?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Beautiful Image" src="./../images/resize_thumb_ambiance_amazing_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="low quality Image" src="./../images/resize_thumb_ambiance_bad_1024.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The Image is of same place taken with different lightening, angle, adjusted contrast. And it is obvious that image on the left has better aesthetic attire.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Significance of Image Aesthetics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For a platform especially that serves media content, one of the crucial aspect is to show high quality content. With social sites and the given ‘selfie’ trend, we are generating huge amount of data in the form of either images or videos.
 Having a track on the quality will always be helpful.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Curated Content&lt;/th&gt;
&lt;th&gt;User Generated Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Curated Content" src="./../images/resize_thumb_Screen_Shot_2017_04_15_at_11_18_25_AM_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="User Generated Content" src="./../images/resize_thumb_Screen_Shot_2017_04_15_at_11_30_00_AM_1024.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;strong&gt;Can we model such Human Perception?&lt;/strong&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;strong&gt;Deep learning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The topic needs no introduction. It’s a revolution especially in the image classification domain since the last 5 years. With “Alexnet” winning the Image-Net competition, improving error rate with a huge margin acted as spark in the field. Since then, CNN has many state of the arts on its name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network architecture of Alexnet.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alexnet architecture" src="./../images/thumb_CNN_image_1024.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The first layer is input, where input in fed to the network. We can see there pooling operations, convolution operations finally followed by a fully connected layer 
and final softmax layer so that we get values as the probability for each class we label.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Fixed size input constraint&lt;/strong&gt;&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Input Layer&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Input Layer" src="./../images/thumb_input_CNN_image_1024.jpg"&gt;&lt;/td&gt;
&lt;td&gt;Input image &lt;strong&gt;re-sized to 224 * 224&lt;/strong&gt; irrespective of original image shape.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We always resize the input feature vector. If the image is larger, image is cropped 
or pad image if image dimensions are smaller, to get a fixed size input to fed the network&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;The Mountains&lt;/th&gt;
&lt;th&gt;Qutub Minar&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Mountains" src="./../images/iceland-blue-lagoon-and-snow-mountains-landscape-header.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Qutub Minar" src="./../images/qutub-minar-minaret.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The above two images are &lt;strong&gt;beautiful in their original aspect ratio.&lt;/strong&gt; 
What happens if we re-size the image to a fixed size of 224 * 224? 
Certainly the image will &lt;strong&gt;loose all it’s original aesthetic value!&lt;/strong&gt; 
From Landscape to Squared size. All damage is done. The original image composition is lost when image is re-sized.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Demystifying the Network Architecture&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Network Architecture" src="./../images/thumb_CNN_image_1024.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Let’s unveil the hidden layers! So, we can see that after the input , there are few layers of &lt;strong&gt;Operations&lt;/strong&gt;. 
The operations are either &lt;strong&gt;Max-pooling&lt;/strong&gt; or convolving with a filter i.e. &lt;strong&gt;Convolution&lt;/strong&gt;.
So why fixed size of input is required at all then?&lt;br&gt;
It’s because of the &lt;strong&gt;Fully Connected Layer&lt;/strong&gt; just before the outputs. 
Fully Connected Layers are in the network for non-linear combination of feature extracted before in convolution network.&lt;/p&gt;
&lt;p&gt;Let's understand bit by bit.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Max Pooling&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Max pooling are there for &lt;strong&gt;Down-sampling&lt;/strong&gt; the feature space while maintain the spatial information
Max Pooling in action&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max Pooling" src="./../images/cropped_max_pooling.gif"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Spatial Pyramid Pooling&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In spp, an image is divided into bins. Each bin is pooled in its turn. As the number of bins are fixed, 
we always get the &lt;strong&gt;Fixed Shape Output&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Spp operation in action&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Pyramid Pooling" src="./../images/sppp_2_cropped.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Pyramid Pooling" src="./../images/bin_4_spp.gif"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Spp Network Architecture&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Spp Network Architecture" src="./../images/network.png"&gt;&lt;/p&gt;
&lt;p&gt;The first network is the traditional CNN , we can see the &lt;strong&gt;Max-pool layer&lt;/strong&gt; just before the fully connected layer.
In the second architecture, the last max pooling layer is &lt;strong&gt;replaced by a Spp layer&lt;/strong&gt;. 
With the &lt;strong&gt;Fixed Bin size (1,2,4)&lt;/strong&gt; we make sure that the fully connected layer gets the fixed shape input.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spp Network Architecture" src="./../images/687474703a2f2f692e696d6775722e636f6d2f5351574a566f442e706e67.png"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Training the Spp-Net&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Training the Spp-Net on &lt;strong&gt;live-dataset&lt;/strong&gt;, very small dataset, about 1K images total, model achieved the accuracy of 75% on training data, 83% on the test data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Training Loss&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Spp-Net Accuracy" src="./../images/training_accuracy.png"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Spp-Net Training Loss" src="./../images/training_loss.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;With Spp in Network&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model learns the scale invariant feature like SIFT(traditional image processing algorithm).&lt;/li&gt;
&lt;li&gt;One of the challenge in text classification with Deep learning is the fixed size feature vector representation of sentence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Interesting Results&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After training model, I experimented for few results. These are the most interesting and promising results I found.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Blurred Cropped Image&lt;/th&gt;
&lt;th&gt;Complete Image&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Blurred Image" src="./../images/low_quality_blurred_image.jpg"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Complete Image" src="./../images/high_quality_blurred_image.jpg"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I recently came across this &lt;a href="http://www.dailymail.co.uk/travel/travel_news/article-4778066/Photography-tips-tricks-Phillip-Haumesser.html"&gt;Photographer&lt;/a&gt; experience from being a amateur to professional.&lt;br&gt;
He proved what difference a change in perspective can make. So, I decided to make my trained model judge for his efficacy.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amateur Click? Yes, it is.&lt;/th&gt;
&lt;th&gt;Pro Click? I am already amazed.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="Beautiful Image" src="../images/amateur_photographer.png"&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="low quality Image" src="../images/professionl_photographer.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The aesthetic trained model has passed him with flying colors in Photographic skills. Well done Phillip Haumesser.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1406.4729"&gt;Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/document/7780429/"&gt;Long Mai, Hailin Jin, Feing Liu, Composition-Preserving Deep Photo Aesthetics Assessment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that I would like to wrap up. Any Questions ?&lt;/p&gt;</content><category term="image quality prediction"></category><category term="spatial pyramid pooling"></category><category term="spp-net"></category><category term="deep learning"></category></entry><entry><title>Language Modeling</title><link href="https://yardstick17.github.io/language-modeling.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/language-modeling.html</id><summary type="html">&lt;p&gt;Language models generally try to compute the probability of a word wtwt given its n−1n−1 previous words, i.e. p(wt|wt−1,⋯wt−n+1)p(wt|wt−1,⋯wt−n+1). &lt;/p&gt;</summary><content type="html">&lt;p&gt;Language models generally try to compute the probability of a word wtwt given its n−1n−1 previous words, i.e. p(wt|wt−1,⋯wt−n+1)p(wt|wt−1,⋯wt−n+1). &lt;/p&gt;</content></entry><entry><title>Learning Semantic Hierarchies viaWord Embeddings</title><link href="https://yardstick17.github.io/learning-semantic-hierarchies-viaword-embeddings.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/learning-semantic-hierarchies-viaword-embeddings.html</id><summary type="html">&lt;p&gt;&lt;a href="https://www.google.co.in/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiP5oKIq_LUAhVJMo8KHSF8B3QQFgglMAA&amp;amp;url=http%3A%2F%2Fir.hit.edu.cn%2F~car%2Fpapers%2Facl14embedding.pdf&amp;amp;usg=AFQjCNF-HUqFNmAAAcqXmGMi773iWNLRVQ"&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, Ting Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper proposes a methodology to build relationships(hierarchies) between the words. In its core, it
 checks whether two words share a hypernym–hyponym relationship using the word embeddings.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.google.co.in/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiP5oKIq_LUAhVJMo8KHSF8B3QQFgglMAA&amp;amp;url=http%3A%2F%2Fir.hit.edu.cn%2F~car%2Fpapers%2Facl14embedding.pdf&amp;amp;usg=AFQjCNF-HUqFNmAAAcqXmGMi773iWNLRVQ"&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, Ting Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper proposes a methodology to build relationships(hierarchies) between the words. In its core, it
 checks whether two words share a hypernym–hyponym relationship using the word embeddings.&lt;/p&gt;</content></entry><entry><title>Noise-Contrastive Estimation and its Generalizations</title><link href="https://yardstick17.github.io/noise-contrastive-estimation-and-its-generalizations.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/noise-contrastive-estimation-and-its-generalizations.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www2.warwick.ac.uk/fac/sci/statistics/crism/workshops/estimatingconstants/gutmann.pdf"&gt;Paper Link&lt;/a&gt;
&lt;strong&gt;Authors:&lt;/strong&gt; Michael Gutmann
&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper is second in the series by Mikolov et al. of &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2009/mikolov_ic2009_nnlm_4.pdf"&gt;Statistical Language Models Based on Neural Networks&lt;/a&gt;
It is an extension to the proposed Skip-gram model in terms of quality and training speed. It also incorporates methods for Phrase representation and explores …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www2.warwick.ac.uk/fac/sci/statistics/crism/workshops/estimatingconstants/gutmann.pdf"&gt;Paper Link&lt;/a&gt;
&lt;strong&gt;Authors:&lt;/strong&gt; Michael Gutmann
&lt;strong&gt;Aim:&lt;/strong&gt;  &lt;br&gt;
This Paper is second in the series by Mikolov et al. of &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2009/mikolov_ic2009_nnlm_4.pdf"&gt;Statistical Language Models Based on Neural Networks&lt;/a&gt;
It is an extension to the proposed Skip-gram model in terms of quality and training speed. It also incorporates methods for Phrase representation and explores the 
interesting property of Skip-gram model.
&lt;a href="http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf"&gt;Notes on Noise Contrastive Estimation and Negative Sampling&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Review Highlight</title><link href="https://yardstick17.github.io/review-highlight.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/review-highlight.html</id><summary type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</content></entry><entry><title>Word Embeddings - Skip-gram model</title><link href="https://yardstick17.github.io/word-embeddings-skip-gram-model.html" rel="alternate"></link><published>2017-04-01T00:00:00+05:30</published><updated>2017-04-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-04-01:/word-embeddings-skip-gram-model.html</id><summary type="html">&lt;p&gt;Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual 
raw pixel-intensities for image data, or e.g. power spectral density coefficients for audio data. For tasks like object 
or speech recognition we know that all the information required to successfully perform the task …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual 
raw pixel-intensities for image data, or e.g. power spectral density coefficients for audio data. For tasks like object 
or speech recognition we know that all the information required to successfully perform the task is encoded in the data 
(because humans can perform these tasks from the raw data). However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs' (such that they are both animals, four-legged, pets, etc.). Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. 
Using vector representations can overcome some of these obstacles. &lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;References&lt;/strong&gt;  FIX LINKS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1406.4729"&gt;Hierarchical Probabilistic Neural Network Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;Distributed Representations of Words and Phrases and their Composition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/document/7780429/"&gt;Noise-Contrastive Estimation and its Generalizations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that I would like to wrap up. Any Questions ?&lt;/p&gt;</content></entry><entry><title>Image Classification</title><link href="https://yardstick17.github.io/image-classification.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/image-classification.html</id><summary type="html">&lt;p&gt;A Deep learning approach to classify image into four categories - Food, Ambiance, Menu and Human.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Deep learning approach to classify image into four categories - Food, Ambiance, Menu and Human.&lt;/p&gt;</content></entry><entry><title>Image Similarity</title><link href="https://yardstick17.github.io/image-similarity.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/image-similarity.html</id><summary type="html">&lt;p&gt;A Deep learning approach to identify a probable image plagiarism.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Deep learning approach to identify a probable image plagiarism.&lt;/p&gt;</content></entry><entry><title>Optimizers in Deep Learning</title><link href="https://yardstick17.github.io/optimizers-in-deep-learning.html" rel="alternate"></link><published>2017-03-01T00:00:00+05:30</published><updated>2017-03-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-03-01:/optimizers-in-deep-learning.html</id><summary type="html">&lt;p&gt;The fundamental of any Machine learning models is defining a cost function or loss function. Optimizing over data-set i.e. minimizing our
cost function or loss enables us to learn the model. The technique of efficiently minimizing our cost function is done using the
well defined algorithms under the premise …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The fundamental of any Machine learning models is defining a cost function or loss function. Optimizing over data-set i.e. minimizing our
cost function or loss enables us to learn the model. The technique of efficiently minimizing our cost function is done using the
well defined algorithms under the premise of Optimizers.  &lt;/p&gt;</content></entry><entry><title>Testing in Python</title><link href="https://yardstick17.github.io/testing-in-python.html" rel="alternate"></link><published>2017-02-01T00:00:00+05:30</published><updated>2017-02-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-02-01:/testing-in-python.html</id><summary type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</content></entry><entry><title>Data Pipelining in Python: Luigi</title><link href="https://yardstick17.github.io/data-pipelining-in-python-luigi.html" rel="alternate"></link><published>2017-01-01T00:00:00+05:30</published><updated>2017-01-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-01-01:/data-pipelining-in-python-luigi.html</id><summary type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</content></entry><entry><title>Save and Restore Tensorflow Models</title><link href="https://yardstick17.github.io/save-and-restore-tensorflow-models.html" rel="alternate"></link><published>2017-01-01T00:00:00+05:30</published><updated>2017-01-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-01-01:/save-and-restore-tensorflow-models.html</id><summary type="html">&lt;p&gt;Tensorflow is a Deep Learning Library developed by Google.
I will write soon. Stay Tuned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tensorflow is a Deep Learning Library developed by Google.
I will write soon. Stay Tuned.&lt;/p&gt;</content></entry><entry><title>Word Embeddings Using Word2vec Trained On Tensorflow</title><link href="https://yardstick17.github.io/word-embeddings-using-word2vec-trained-on-tensorflow.html" rel="alternate"></link><published>2017-01-01T00:00:00+05:30</published><updated>2017-01-01T00:00:00+05:30</updated><author><name>Amit Kushwaha</name></author><id>tag:yardstick17.github.io,2017-01-01:/word-embeddings-using-word2vec-trained-on-tensorflow.html</id><summary type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I will write soon. Stay Tuned.&lt;/p&gt;</content></entry></feed>